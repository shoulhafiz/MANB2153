{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1 0 1 1 1 0 1 0 2 2 1 0 0 1 0 1 0 2 1 2 1 1 1 2 1 0 0 1 0 0 0]\n",
      "Training Accuracy:  0.7676056338028169\n",
      "Test Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "# Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# Create arrays for the features and the response variable\n",
    "y = wine.target\n",
    "X = wine.data\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state= 0)\n",
    "\n",
    "# Create a k-NN classifier with 6 neighbors: knn\n",
    "classifier = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "#Compute accuracy on the training set\n",
    "training_accuracy = classifier.score(X_train, y_train)\n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "#Compute accuracy on the testing set\n",
    "test_accuracy = classifier.score(X_test, y_test)\n",
    "print(\"Test Accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 1 0 0 0 0 1 1 1 1 1 1 2 0 0 1 0 0 0]\n",
      "Training Accuracy:  1.0\n",
      "Test Accuracy:  0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "## decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# Create arrays for the features and the response variable\n",
    "y = wine.target\n",
    "X = wine.data\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state= 0)\n",
    "\n",
    "# Create a k-NN classifier with 6 neighbors: knn\n",
    "classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "#Compute accuracy on the training set\n",
    "training_accuracy = classifier.score(X_train, y_train)\n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "#Compute accuracy on the testing set\n",
    "test_accuracy = classifier.score(X_test, y_test)\n",
    "print(\"Test Accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 0 1 1 0 2 1 1 2 2 0 0 2 1 0 0 2 0 0 0 0 1 1 1 1 1 1 2 0 0 1 0 0 0]\n",
      "Training Accuracy:  0.9859154929577465\n",
      "Test Accuracy:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "## naive bayes\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Importing the dataset\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# Create arrays for the features and the response variable\n",
    "y = wine.target\n",
    "X = wine.data\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "#Compute accuracy on the training set\n",
    "training_accuracy = classifier.score(X_train, y_train)\n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "#Compute accuracy on the testing set\n",
    "test_accuracy = classifier.score(X_test, y_test)\n",
    "print(\"Test Accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing exercises: find 10 papers that have applied kNN, Decision Tree and Naive Bayes to solve their problems. For each paper, explain the problems they are solving, the techniques and the data that were used in the paper. Write this in your Jupyter Notebook.\n",
    "\n",
    "1)\tGasoline classification using near infrared (NIR) spectroscopy data: Comparison of multivariate techniques\n",
    "Cite : Balabin, R. M., Safieva, R. Z., & Lomakina, E. I. (2010). Gasoline classification using near infrared (NIR) spectroscopy data: Comparison of multivariate techniques. Analytica Chimica Acta, 671(1-2), 27-35.\n",
    "Problem: Complicated classification of gasoline  in a complicated real process, where on-line measuring is important to monitor the quality of products.\n",
    "Techniques: KNN, SVM, and PNN techniques\n",
    "Data:Three sets of near infrared (NIR) spectra (450, 415,and 345 spectra) were used for classification of gasolines into 3, 6, and 3\n",
    "\n",
    "2)\tNew colour SIFT descriptors for image classification with applications to biometrics\n",
    "Cite : Verma, A., Liu, C., & Jia, J. (2011). New colour SIFT descriptors for image classification with applications to biometrics. International Journal of Biometrics, 3(1), 56.\n",
    "Problem: Performance of image classifications under different colour spaces (illumination, shading, highlights)\n",
    "Techniques: KNN\n",
    "Data: 20 image categories from two large scale, grand challenge datasets: the Caltech 256 database and the UPOL Iris database.\n",
    "\n",
    "3)\tEnhancing WO3 gas sensor selectivity using a set of pollutant detection classifiers\n",
    "Cite : Rabeb Faleh, Sami Gomri, Mehdi Othman, Khalifa Aguir, Abdennaceur Kachouri, (2018) \"Enhancing WO3 gas sensor selectivity using a set of pollutant detection classifiers\", Sensor Review, Vol. 38 Issue: 1, pp.65-73, https://doi.org/10.1108/SR-12-2016-0273\n",
    "Problem: Reduce pollutant by solving the problem of cross-selectivity of gases in electronic nose (E-nose)\n",
    "Techniques: KNN & svm\n",
    "Data: three WO3 sensors E-nose system was used for data acquisition to detect three gases, namely, ozone,ethanol and acetone\n",
    "\n",
    "4)\tAutomated web usage data mining and recommendation system using K-Nearest Neighbor (KNN) classification method \n",
    "Cite : Adeniyi, D. A., Wei, Z., & Yongquan, Y. (2016). Automated web usage data mining and recommendation system using K-Nearest Neighbor (KNN) classification method. Applied Computing and Informatics, 12(1), 90-108.\n",
    "Problem: strenuous and time-consuming task in finding the right product or information on the site \n",
    "Techniques: KNN\n",
    "Data: user behavior through his/her click stream data on the newly developed Really Simple Syndication (RSS) reader website.\n",
    "5)\tIntrusion detection using naive Bayes classifier with feature reduction.\n",
    "Cite : Mukherjee, S., & Sharma, N. (2012). Intrusion detection using naive Bayes classifier with feature reduction. Procedia Technology, 4, 119-128.\n",
    "Problem: Degraded performance of Intrusion Detection Systems (IDS)\n",
    "Techniques: naive bayes\n",
    "Data: NSL-KDD dataset contains one type of normal data and 22 different types of attacks which falls into one of four categories. The dataset contains 62,986 observations.\n",
    "\n",
    "\n",
    "6)\tShallow learning model for diagnosing neuro muscular disorder from splicing variants\n",
    "Cite: Sathyavikasini Kalimuthu, Vijaya Vijayakumar, (2017) \"Shallow learning model for diagnosing neuro muscular disorder from splicing variants\", World Journal of Engineering, Vol. 14 Issue: 4, pp.329-336, https://doi.org/10.1108/WJE-09-2016-0075\n",
    "Problem: Diagnosing genetic neuromuscular disorder such as muscular dystrophy is complicated when the imperfection occurs while splicing\n",
    "\n",
    "Techniques:  Naïve Bayes, decision tree, K-nearest neighbor and SVM\n",
    "\n",
    "Data: The synthetic mutational gene sequences are created, as the diseased gene sequences are not readily obtainable for this intricate disease.\n",
    "\n",
    "7)\tA Comparison of Event Models for Naive Bayes Text Classification\n",
    "Problem: To clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora.\n",
    "Techniques: Naïve-Bayes\n",
    "Data: The web pages pointed to by the Yahoo! ‘Science’ hierarchy were gathered in July 1997\n",
    "\n",
    "8)\tForecasting copper prices by decision tree learning\n",
    "Cite: Liu, C., Hu, Z., Li, Y., & Liu, S. (2017). Forecasting copper prices by decision tree learning. Resources Policy, 52, 427-434.\n",
    "Problem: Unpredictable copper prices\n",
    "Techniques: Decision tree\n",
    "Data: Dataset of copper prices\n",
    "\n",
    "9)\tPerformance Analysis of Naive Bayes and J48 Classification Algorithm for Data Classification \n",
    "Cite: Patil, T. R., & Sherekar, S. S. (2013). Performance analysis of Naive Bayes and J48 classification algorithm for data classification. International journal of computer science and applications, 6(2), 256-261.\n",
    "Problem: performance evaluation based on the correct and incorrect instances of data classification\n",
    "Techniques: Decision tree & Naive-Bayes\n",
    "Data: bank dataset to maximize true positive rate and minimize false positive rate of defaulters\n",
    "\n",
    "10)\tApplication of decision tree technology for image classification using remote sensing data\n",
    "Cite: Yang, C. C., Prasher, S. O., Enright, P., Madramootoo, C., Burgess, M., Goel, P. K., & Callum, I. (2003). Application of decision tree technology for image classification using remote sensing data. Agricultural Systems, 76(3), 1101-1117.\n",
    "Problem: The accuracy crop image classification\n",
    "Techniques: Decision Tree\n",
    "Data: hyperspectral reflectance measurements per plot were taken randomly to obtain a total of 90 measurements\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
